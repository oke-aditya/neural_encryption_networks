{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ICADCML 2021 A Novel Approach to Encrypt Data using Deep Neural Networks In this paper we propose a novel approach to Encrpyt data using Deep Neural Networks. We propose an Autoencoder techinque which can sucessfully encrypt and decrypt data. We secure this method using keys by ensembling autoencoders. Project layout This is how the project is structured. We also provide Colab Notebooks that can be used to reproduce our results. \u251c\u2500\u2500 docs # Documentation files built using mkdocs \u251c\u2500\u2500 models # Models that we trained \u251c\u2500\u2500 neural_encryption_networks # This folder contains all code. \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 notebooks # Reproducible notebooks. \u2502 \u2514\u2500\u2500 src # Python scripts. \u2514\u2500\u2500 requirements.txt # To install stuff. Colab Notebooks We provide Colab notebooks to directly play with. The are available in in this folder too. Encrypting with Deep Neural Networks Ensemble Network for Keys Runing locally Install the requirments by running pip install -r requirements.txt The code uses Tensorflow 2.4. And is Tested on Python 3.6+ We recommend using virtual environements using Conda or similar to avoid conflicts. cd neural_encryption_networks/src This folder contains all the code you need ! Citation We will provide a Bibtex entry soon. For now people can cite this GitHub repository. @misc{, author = {Aditya Oke, Abhishek Kushwaha, Manan Agrawal, Utkrisht Sahai, Rahul Raman}, title = {A Novel Approach to Encrypt Data Using Deep Neural Networks}, year = {2021}, publisher = {}, journal = {}, howpublished = {https://github.com/oke-aditya/neural_encryption_networks/}, }","title":"Home"},{"location":"#icadcml-2021-a-novel-approach-to-encrypt-data-using-deep-neural-networks","text":"In this paper we propose a novel approach to Encrpyt data using Deep Neural Networks. We propose an Autoencoder techinque which can sucessfully encrypt and decrypt data. We secure this method using keys by ensembling autoencoders.","title":"ICADCML 2021 A Novel Approach to Encrypt Data using Deep Neural Networks"},{"location":"#project-layout","text":"This is how the project is structured. We also provide Colab Notebooks that can be used to reproduce our results. \u251c\u2500\u2500 docs # Documentation files built using mkdocs \u251c\u2500\u2500 models # Models that we trained \u251c\u2500\u2500 neural_encryption_networks # This folder contains all code. \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 notebooks # Reproducible notebooks. \u2502 \u2514\u2500\u2500 src # Python scripts. \u2514\u2500\u2500 requirements.txt # To install stuff.","title":"Project layout"},{"location":"#colab-notebooks","text":"We provide Colab notebooks to directly play with. The are available in in this folder too. Encrypting with Deep Neural Networks Ensemble Network for Keys","title":"Colab Notebooks"},{"location":"#runing-locally","text":"Install the requirments by running pip install -r requirements.txt The code uses Tensorflow 2.4. And is Tested on Python 3.6+ We recommend using virtual environements using Conda or similar to avoid conflicts. cd neural_encryption_networks/src This folder contains all the code you need !","title":"Runing locally"},{"location":"#citation","text":"We will provide a Bibtex entry soon. For now people can cite this GitHub repository. @misc{, author = {Aditya Oke, Abhishek Kushwaha, Manan Agrawal, Utkrisht Sahai, Rahul Raman}, title = {A Novel Approach to Encrypt Data Using Deep Neural Networks}, year = {2021}, publisher = {}, journal = {}, howpublished = {https://github.com/oke-aditya/neural_encryption_networks/}, }","title":"Citation"},{"location":"autoencoders/","text":"Autoencoders A feed forward autoencoder is a Neural Network containing two feedforward networks. One is encoder and other is decoder. The encoder is a feedforward network which converts the given input into a latent representation. The decoder decodes the latent representation to reconstruct the orignal data. Both are feed-forward networks which contain several non linear layers. These decrease the total input representation dimension to latent representation dimensions. Latent Representation Latent Representation is a joint representation learnt commonly by both encoder and decoder. This is unique to encoder as well as decoder. Learning this latent representation allows us to uniquely encrypt data. Training Autoencoder To trian an autoencoder we should simulatanously train the encoder as well as the decoder. While the encoder tries to minimize the given input representation, the decoder tries to recreate encoder inputs. In this process they both learn the latent representation. A Small Hint of Supervision The latent representation can be completely random. Since, it is jointly learnt to minize the loss. We can add a small hint of supervision by passing labels for encoders and decoders. This makes the latent distribution a bit consistent. This helps in deterministic execution of AutoEncoder.","title":"Autoencoders"},{"location":"autoencoders/#autoencoders","text":"A feed forward autoencoder is a Neural Network containing two feedforward networks. One is encoder and other is decoder. The encoder is a feedforward network which converts the given input into a latent representation. The decoder decodes the latent representation to reconstruct the orignal data. Both are feed-forward networks which contain several non linear layers. These decrease the total input representation dimension to latent representation dimensions.","title":"Autoencoders"},{"location":"autoencoders/#latent-representation","text":"Latent Representation is a joint representation learnt commonly by both encoder and decoder. This is unique to encoder as well as decoder. Learning this latent representation allows us to uniquely encrypt data.","title":"Latent Representation"},{"location":"autoencoders/#training-autoencoder","text":"To trian an autoencoder we should simulatanously train the encoder as well as the decoder. While the encoder tries to minimize the given input representation, the decoder tries to recreate encoder inputs. In this process they both learn the latent representation.","title":"Training Autoencoder"},{"location":"autoencoders/#a-small-hint-of-supervision","text":"The latent representation can be completely random. Since, it is jointly learnt to minize the loss. We can add a small hint of supervision by passing labels for encoders and decoders. This makes the latent distribution a bit consistent. This helps in deterministic execution of AutoEncoder.","title":"A Small Hint of Supervision"},{"location":"keys/","text":"Keys For Secure Encryption For secure encryption algorithm one need keys; Namely a public key and a private key. Public Key The public key is the number of encrypters used and their ordering. This enables our decryptors to know the decryption ordering. The public key does not risk our encryption algorithm. It does not expose the mapping of encrypters. Private Key The latent distribution mapping is a private key. It differs in number of encoded bits for each encryter-decrypter set, as well as the learnt latent distribution varies for each encoder and decoder. Thus we can create a safe 2 key encrpytion-decryption mechanism.","title":"Securing With Keys"},{"location":"keys/#keys-for-secure-encryption","text":"For secure encryption algorithm one need keys; Namely a public key and a private key.","title":"Keys For Secure Encryption"},{"location":"keys/#public-key","text":"The public key is the number of encrypters used and their ordering. This enables our decryptors to know the decryption ordering. The public key does not risk our encryption algorithm. It does not expose the mapping of encrypters.","title":"Public Key"},{"location":"keys/#private-key","text":"The latent distribution mapping is a private key. It differs in number of encoded bits for each encryter-decrypter set, as well as the learnt latent distribution varies for each encoder and decoder. Thus we can create a safe 2 key encrpytion-decryption mechanism.","title":"Private Key"},{"location":"mapping/","text":"Mapping to learn Latent Distribution. To give Small Hint of Supervision we use mapping. Mapping helps to create a almost consistent latent distribution, which is learnt by encoder. To create mapping, we use hashmaps which are generated by following algorithm. Pseudocode 1. Store all the possible writable characters in a list characters = [chr(i) for i in range(32, 123)] 2. Generate a random integer between a big range to be assigned 3. For 100 iterations: Picking out a random integer from a large range(around 2 power 18) Appending the binary representation of the random integer End for 4. Shuffle all the sampled numbers that were picked. 5. Assign individual mapping of the bits to the corresponding alphabets in a random order Code Code for above pseudocode is as follows def hash_map_generator():ters # Take all UTF-8 supported charac characters = [chr(i) for i in range(32, 123)] # characters.append(' ') hash_map_values = [] # Generate the random integer for i in range(100): a = random.randint(2 ** 18, 2 ** 19) hash_map_values.append(int(bin(a)[2:])) # print(hash_map_values) # Shuffle the mapping. random.shuffle(characters) random.shuffle(hash_map_values) hash_map = dict(zip(characters, hash_map_values)) # print(hash_map) return hash_map The above code is located here Once we create Mapping we can Train AutoEncoders.","title":"Mapping"},{"location":"mapping/#mapping-to-learn-latent-distribution","text":"To give Small Hint of Supervision we use mapping. Mapping helps to create a almost consistent latent distribution, which is learnt by encoder. To create mapping, we use hashmaps which are generated by following algorithm.","title":"Mapping to learn Latent Distribution."},{"location":"mapping/#pseudocode","text":"1. Store all the possible writable characters in a list characters = [chr(i) for i in range(32, 123)] 2. Generate a random integer between a big range to be assigned 3. For 100 iterations: Picking out a random integer from a large range(around 2 power 18) Appending the binary representation of the random integer End for 4. Shuffle all the sampled numbers that were picked. 5. Assign individual mapping of the bits to the corresponding alphabets in a random order","title":"Pseudocode"},{"location":"mapping/#code","text":"Code for above pseudocode is as follows def hash_map_generator():ters # Take all UTF-8 supported charac characters = [chr(i) for i in range(32, 123)] # characters.append(' ') hash_map_values = [] # Generate the random integer for i in range(100): a = random.randint(2 ** 18, 2 ** 19) hash_map_values.append(int(bin(a)[2:])) # print(hash_map_values) # Shuffle the mapping. random.shuffle(characters) random.shuffle(hash_map_values) hash_map = dict(zip(characters, hash_map_values)) # print(hash_map) return hash_map The above code is located here Once we create Mapping we can Train AutoEncoders.","title":"Code"},{"location":"neural_enc/","text":"Encrypting Data Using Auto-Encoders The overall procedure is as below. Training Set And Mapping We generate a very long string (Around 1000 - 1500 words) for training. This is again generated by randomly shuffling characters from UTF-8 printable characters. Code for same is below. def sequence(word_length): # All utf-8 characters characters = [chr(i) for i in range(32, 123)] # Shuffle them random.shuffle(characters) # Add to string word = \"\".join(characters[:word_length]) return word def random_paragraph_generator(): # Generate random number of words no_of_words = random.randint(1000, 1500) para = \"\" # Random Word length. for i in range(no_of_words): word_length = random.randint(1, 10) word = sequence(word_length) para += word para += \" \" # Encode in UTF-8 and return return para.encode(\"utf-8\") We now have a long paragraph. We simply one hot encode the paragraph. Here our hashmap is slight hint of supervision to learn latent distribution. To learn a latent distribution, we train encoder and decoder. Encoder and Decoder The inputs of encoder are one-hot encoded data, and is hinted with labels of hashmap. Thus we train encoder from one-hot encoded data to hashmap, learning a latent distribution (function approximation) The outputs of encoder are passed as inputs to decoder. The decoder learns to reconstruct back the given plain text from latent distribution. The decoder outputs back one-hot encoded data, which is very simple to decode. We implemented this logic in tensorflow.keras API. Training We train Encoder and Decoder with Mean Squared Error Loss, Adam Optimizer. We keep a small learning rate around 1e-3 . Since there are not many gradients to compute, batch size is kept dynamic as per tensorflow using None . Training both Encoder and Decoder Jointly takes around 3-4 mins over CPU and 2 mins over GPU. Ensembling Networks Once we have trained one set of enocder (encrypter) and decoder (decrypter). We can use similar configuration and train another. We trained 2 such set of encrypter and decrypters. Both had slightly different mapping, created by mapping algorithm. Encrypter, Decrypter small network had hashmap with encoding size 32 and a larger with encoding size 56. This allows us to create secure networks, by ensembling them. Encrypting With Ensemble Networks We have 2 sets of encrypters and decrypters this case. Now we can use each of them to encrypt new data, we can mark each of them with IDs We create a cryptographically secure random number which will allocate each ID to parts of long text data. Thus networks can encrypt the data allocated to them. The IDs allocated to them is used as a public key. More on Keys in next page.","title":"Encrypting Data"},{"location":"neural_enc/#encrypting-data-using-auto-encoders","text":"The overall procedure is as below.","title":"Encrypting Data Using Auto-Encoders"},{"location":"neural_enc/#training-set-and-mapping","text":"We generate a very long string (Around 1000 - 1500 words) for training. This is again generated by randomly shuffling characters from UTF-8 printable characters. Code for same is below. def sequence(word_length): # All utf-8 characters characters = [chr(i) for i in range(32, 123)] # Shuffle them random.shuffle(characters) # Add to string word = \"\".join(characters[:word_length]) return word def random_paragraph_generator(): # Generate random number of words no_of_words = random.randint(1000, 1500) para = \"\" # Random Word length. for i in range(no_of_words): word_length = random.randint(1, 10) word = sequence(word_length) para += word para += \" \" # Encode in UTF-8 and return return para.encode(\"utf-8\") We now have a long paragraph. We simply one hot encode the paragraph. Here our hashmap is slight hint of supervision to learn latent distribution. To learn a latent distribution, we train encoder and decoder.","title":"Training Set And Mapping"},{"location":"neural_enc/#encoder-and-decoder","text":"The inputs of encoder are one-hot encoded data, and is hinted with labels of hashmap. Thus we train encoder from one-hot encoded data to hashmap, learning a latent distribution (function approximation) The outputs of encoder are passed as inputs to decoder. The decoder learns to reconstruct back the given plain text from latent distribution. The decoder outputs back one-hot encoded data, which is very simple to decode. We implemented this logic in tensorflow.keras API.","title":"Encoder and Decoder"},{"location":"neural_enc/#training","text":"We train Encoder and Decoder with Mean Squared Error Loss, Adam Optimizer. We keep a small learning rate around 1e-3 . Since there are not many gradients to compute, batch size is kept dynamic as per tensorflow using None . Training both Encoder and Decoder Jointly takes around 3-4 mins over CPU and 2 mins over GPU.","title":"Training"},{"location":"neural_enc/#ensembling-networks","text":"Once we have trained one set of enocder (encrypter) and decoder (decrypter). We can use similar configuration and train another. We trained 2 such set of encrypter and decrypters. Both had slightly different mapping, created by mapping algorithm. Encrypter, Decrypter small network had hashmap with encoding size 32 and a larger with encoding size 56. This allows us to create secure networks, by ensembling them.","title":"Ensembling Networks"},{"location":"neural_enc/#encrypting-with-ensemble-networks","text":"We have 2 sets of encrypters and decrypters this case. Now we can use each of them to encrypt new data, we can mark each of them with IDs We create a cryptographically secure random number which will allocate each ID to parts of long text data. Thus networks can encrypt the data allocated to them. The IDs allocated to them is used as a public key. More on Keys in next page.","title":"Encrypting With Ensemble Networks"},{"location":"references/","text":"Additional References These contain more detailed explaintions on concepts that we have used. Autoencoders: Nice set of video lectures from NPTEL which describe how Auto-enocders work in detail. Crpytography: You can have a look at Coursera Course from Stanford University. Credits Autoencoder Image in docs is taken from Neural Network Projects with Python, James Loy This is docs built using mkdocs.org . Google Colab for model training and notebooks ! Citations Volna, Eva and Kotyrba, Martin and Kocian, Vaclav and Janosek, Michal, \u201cCryp- tography Based On Neural Network,\u201c10.7148/2012-0386-0391, May 2012. Martin Abadi and David G. Andersen, \u201c Learning to Protect Communications using Adverserial Neural Cryptography, \u201c arXiv:1610.06918v1, October 2016. Dayiheng Liu, J.Lv, Xiaofeng Qi and Jiangshu Wei, \u201c A neural words encoding model, \u201c 2016 International Joint Conference on Neural Networks (IJCNN), pp 532\u2013536, July 2016. P. Saraswat, K. Garg, K. Oka, R. Tripathi and A. Agarwal, \u201cEncryption Algorithm Based on Neural Network, vol \u201c, 2019 4th International Conference on Internet of Things: Smart Innovation and Usages (IoT-SIU),pp 1\u20135, April 2019. Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert and Jonathan Passerat-Palmbach, \u201c A generic framework for privacy pre- serving deep learning \u201c, arXiv:1811.04017, 2018. Ajit Singh and Aarti nandal \u201c Neural Cryptography for Secret Key Exchange and Encryption with AES \u201c, International Journal of Advanced Research in Computer Science and Software Engineering, ISBN 2277 128X, Volume 3, Issue 5,pp 376-381, May 2013. Qutaiba Ali and Shefa A. Dawwd, \u201c On demand ciphering engine using artificial neural network, \u201c Int. Arab J. Inf. Technol., Volume 13, pp 462\u2013470, 2016 Shweta Joshi and Vishwanath Udupi and Dipesh Joshi, \u201cA novel neural network approach for digital image data encryption/decryption \u201c, 2012 International Con- ference on Power, Signals, Controls and Computation, pp 1\u20134, 2012. R. Yegireddi and R. K. Kumar \u201cA survey on conventional encryption algorithms of Cryptography\u201c, 2016 International Conference on ICT in Business Industry Gov- ernment (ICTBIG), pp 1\u20134, 2016. Mushtaq, Muhammad and Jamel, Sapiee and Disina, Abdulkadir and Pindar, Zahraddeen and Shakir, Nur and Mat Deris, Mustafa, \u201c A Survey on the Cryp- tographic Encryption Algorithms \u201c, International Journal of Advanced Computer Science and Applications, Volume 8, pp 333\u2013344, November 2017. F. J. D\u2019souza and D. Panchal, \u201c Advanced encryption standard (AES) security en- hancement using hybrid approach \u201c, 2017 International Conference on Computing, Communication and Automation (ICCCA), pp 647\u2013652, 2017. N. Provos, \u201dA Virtual Honeypot Framework\u201d, Proc. Usenix Security Conf., 2004.","title":"References"},{"location":"references/#additional-references","text":"These contain more detailed explaintions on concepts that we have used.","title":"Additional References"},{"location":"references/#autoencoders","text":"Nice set of video lectures from NPTEL which describe how Auto-enocders work in detail.","title":"Autoencoders:"},{"location":"references/#crpytography","text":"You can have a look at Coursera Course from Stanford University.","title":"Crpytography:"},{"location":"references/#credits","text":"Autoencoder Image in docs is taken from Neural Network Projects with Python, James Loy This is docs built using mkdocs.org . Google Colab for model training and notebooks !","title":"Credits"},{"location":"references/#citations","text":"Volna, Eva and Kotyrba, Martin and Kocian, Vaclav and Janosek, Michal, \u201cCryp- tography Based On Neural Network,\u201c10.7148/2012-0386-0391, May 2012. Martin Abadi and David G. Andersen, \u201c Learning to Protect Communications using Adverserial Neural Cryptography, \u201c arXiv:1610.06918v1, October 2016. Dayiheng Liu, J.Lv, Xiaofeng Qi and Jiangshu Wei, \u201c A neural words encoding model, \u201c 2016 International Joint Conference on Neural Networks (IJCNN), pp 532\u2013536, July 2016. P. Saraswat, K. Garg, K. Oka, R. Tripathi and A. Agarwal, \u201cEncryption Algorithm Based on Neural Network, vol \u201c, 2019 4th International Conference on Internet of Things: Smart Innovation and Usages (IoT-SIU),pp 1\u20135, April 2019. Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert and Jonathan Passerat-Palmbach, \u201c A generic framework for privacy pre- serving deep learning \u201c, arXiv:1811.04017, 2018. Ajit Singh and Aarti nandal \u201c Neural Cryptography for Secret Key Exchange and Encryption with AES \u201c, International Journal of Advanced Research in Computer Science and Software Engineering, ISBN 2277 128X, Volume 3, Issue 5,pp 376-381, May 2013. Qutaiba Ali and Shefa A. Dawwd, \u201c On demand ciphering engine using artificial neural network, \u201c Int. Arab J. Inf. Technol., Volume 13, pp 462\u2013470, 2016 Shweta Joshi and Vishwanath Udupi and Dipesh Joshi, \u201cA novel neural network approach for digital image data encryption/decryption \u201c, 2012 International Con- ference on Power, Signals, Controls and Computation, pp 1\u20134, 2012. R. Yegireddi and R. K. Kumar \u201cA survey on conventional encryption algorithms of Cryptography\u201c, 2016 International Conference on ICT in Business Industry Gov- ernment (ICTBIG), pp 1\u20134, 2016. Mushtaq, Muhammad and Jamel, Sapiee and Disina, Abdulkadir and Pindar, Zahraddeen and Shakir, Nur and Mat Deris, Mustafa, \u201c A Survey on the Cryp- tographic Encryption Algorithms \u201c, International Journal of Advanced Computer Science and Applications, Volume 8, pp 333\u2013344, November 2017. F. J. D\u2019souza and D. Panchal, \u201c Advanced encryption standard (AES) security en- hancement using hybrid approach \u201c, 2017 International Conference on Computing, Communication and Automation (ICCCA), pp 647\u2013652, 2017. N. Provos, \u201dA Virtual Honeypot Framework\u201d, Proc. Usenix Security Conf., 2004.","title":"Citations"}]}